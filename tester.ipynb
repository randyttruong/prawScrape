{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps_folder = 'pickles/experiments/'\n",
    "hash_tables_folder = 'pickles/hash_tables/'\n",
    "hadamard_matrices_folder = 'pickles/hadamard_matrices/'\n",
    "priors_folder = 'pickles/priors/'\n",
    "imgs_folder = 'imgs/'\n",
    "p_omegas_folder = 'pickles/p_omegas/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/pun/.pyenv/versions/3.10.2/lib/python3.10/site-packages (1.26.2)\n",
      "Requirement already satisfied: requests in /Users/pun/.pyenv/versions/3.10.2/lib/python3.10/site-packages (2.31.0)\n",
      "Requirement already satisfied: scipy in /Users/pun/.pyenv/versions/3.10.2/lib/python3.10/site-packages (1.11.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pun/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pun/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from requests) (3.3.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pun/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pun/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from requests) (3.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Users/pun/.pyenv/versions/3.10.2/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install numpy requests scipy\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from scipy.stats import zipfian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddits_ranking(subs_wanted): ## returns dict of [\"subreddit\": number_of_users]\n",
    "    ranking = []\n",
    "    subs_count = 0\n",
    "    for i in range(1, 5):\n",
    "        url = f\"https://www.reddit.com/best/communities/{i}/\"\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            parent_div = soup.find(\"div\", class_=\"community-list\")\n",
    "\n",
    "            if parent_div:\n",
    "                subreddit = parent_div.find_all(\"div\", recursive=False)\n",
    "                for div in subreddit:\n",
    "                    if subs_count == subs_wanted:\n",
    "                        return ranking\n",
    "                    text_div = div.find_all(\"div\")[0]\n",
    "                    sreddit_name = text_div.find_all(\"a\")[0].text.strip()\n",
    "                    h6s = text_div.find_all(\"h6\")\n",
    "                    num_members = int(h6s[1].find(\"faceplate-number\").get(\"number\"))\n",
    "                    ranking.append((sreddit_name[2:], num_members))\n",
    "                    subs_count +=1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "    return ranking\n",
    "\n",
    "def get_subreddits_by_category(subs_wanted, target_category): ## returns dict of [\"category\":[r1,r2,r3],...]\n",
    "    sreddit_in_cat = defaultdict(list)\n",
    "    subs_count = 0\n",
    "\n",
    "    for i in range(1, 5):\n",
    "        url = f\"https://www.reddit.com/best/communities/{i}/\"\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            parent_div = soup.find(\"div\", class_=\"community-list\")\n",
    "\n",
    "            if parent_div:\n",
    "                subreddit = parent_div.find_all(\"div\", recursive=False)\n",
    "                for div in subreddit:\n",
    "                    if subs_count == subs_wanted:\n",
    "                        return sreddit_in_cat\n",
    "                    text_div = div.find_all(\"div\")[0]\n",
    "                    sreddit_name = text_div.find_all(\"a\")[0].text.strip()\n",
    "                    category = text_div.find_all(\"h6\")[0].text.strip()\n",
    "                    if category in target_category:\n",
    "                        sreddit_in_cat[category].append(sreddit_name[2:])\n",
    "                    else:\n",
    "                        sreddit_in_cat[\"Others\"].append(sreddit_name[2:])\n",
    "                    subs_count +=1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "    return sreddit_in_cat\n",
    "\n",
    "def top_k_keys(input_dict, k):\n",
    "    # Count the number of items for each key\n",
    "    counts = {key: len(value) for key, value in input_dict.items()}\n",
    "\n",
    "    # Sort the keys based on counts in descending order and select the top k\n",
    "    top_keys = sorted(counts, key=counts.get, reverse=True)[:k]\n",
    "    return top_keys\n",
    "\n",
    "\n",
    "def get_pool_sizes(categories, sreddit_map):\n",
    "    pool_sizes = [0]*len(categories)\n",
    "    for i in range(len(pool_sizes)):\n",
    "        pool_sizes[i] = len(sreddit_map[categories[i]])\n",
    "    \n",
    "    return pool_sizes\n",
    "\n",
    "def get_prior(rankings, categories):\n",
    "    # Create a mapping from subreddit to user count using rankings\n",
    "    user_count_map = {subreddit: count for subreddit, count in rankings}\n",
    "\n",
    "    # Create a list for ordered subreddits and their user counts based on categories\n",
    "    ordered_subreddits = []\n",
    "    for category in categories.keys():\n",
    "        for subreddit in categories[category]:\n",
    "            if subreddit in user_count_map:  # Check if subreddit is in the rankings\n",
    "                ordered_subreddits.append((subreddit, user_count_map[subreddit]))\n",
    "\n",
    "    # Extract user counts and apply logarithmic scale\n",
    "    user_counts = np.array([count for _, count in ordered_subreddits])\n",
    "    log_counts = np.log(user_counts)\n",
    "    probabilities = log_counts / log_counts.sum()\n",
    "\n",
    "    # Assign probabilities to ordered subreddits\n",
    "    prior = {subreddit: prob for subreddit, prob in zip([sub for sub, _ in ordered_subreddits], probabilities)}\n",
    "\n",
    "    return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_pdf(kappa, n):\n",
    "    universe = np.arange(1, n + 1)\n",
    "    x = zipfian.pmf(universe, kappa, n)\n",
    "    return x / x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick 10 categories with the most subreddits and get their pool sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories:  ['Internet Culture and Memes', 'Gaming', 'Technology', 'Funny/Humor', 'Art', 'Animals and Pets', 'Sports', 'Place', 'Food and Drink', 'Learning and Education', 'Others']\n",
      "Pool sizes:  [115, 88, 58, 57, 52, 42, 41, 34, 32, 30, 451]\n"
     ]
    }
   ],
   "source": [
    "#from preprocessing, these categories has the most subreddits\n",
    "target_category = ['Internet Culture and Memes', 'Gaming', 'Technology', 'Funny/Humor', 'Art', 'Animals and Pets', 'Sports', 'Place', 'Food and Drink', 'Learning and Education'] \n",
    "\n",
    "#map subreddits to each target category, putting other subreddits in 'Others'\n",
    "sreddit_map = get_subreddits_by_category(1000, target_category)\n",
    "all_category = target_category + ['Others']\n",
    "pool_sizes = get_pool_sizes(all_category, sreddit_map)\n",
    "print(\"Categories: \", all_category)\n",
    "print(\"Pool sizes: \", pool_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internet Culture and Memes\n",
      "Gaming\n",
      "Technology\n",
      "Funny/Humor\n",
      "Art\n",
      "Animals and Pets\n",
      "Sports\n",
      "Place\n",
      "Food and Drink\n",
      "Learning and Education\n",
      "Others\n"
     ]
    }
   ],
   "source": [
    "# Ordering the keys of the dictionary to be the same order as the list all_category. This is to ensure Prior[0] and P_Omega[0] refer to the same subreddit\n",
    "ordered_dict = {}\n",
    "for category in all_category:\n",
    "    ordered_dict[category] = sreddit_map[category]\n",
    "for k in ordered_dict.keys():\n",
    "    print(k)\n",
    "rankings = get_subreddits_ranking(1000)\n",
    "prior = get_prior(rankings, ordered_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate P_Omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, m, k, eps = 1000, 1024, 65536, 4\n",
    "kappa = 1.2\n",
    "\n",
    "p_omega = []\n",
    "for category in all_category:  # Ensure this follows the same order as in the prior\n",
    "    num_subreddits = len(sreddit_map[category])\n",
    "    p_omega.extend(zip_pdf(kappa, num_subreddits))\n",
    "\n",
    "# Normalize p_omega (should sum to 1)\n",
    "p_omega = np.array(p_omega) / np.sum(p_omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prior) == len(p_omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
